{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2:  Overview of Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Supervised Learning - use the input(predictor, features) to predict the values of the outputs(responses)\n",
    "* quantitative - continuous variable\n",
    "* qualilative - discrete(categorical) variable\n",
    "* dummy - K binary variables that use to represent K-level categorical variable\n",
    "* OLS, aka ordinary least squares - picked the coefficient $\\beta$ to minimize the residual sum of squares\n",
    "* KNN, aka k-nearest-neigbor Methods - use observations in the training set that closest in input space to a given x to form the predicted response Y\n",
    "* EPE, aka expected prediction error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why k is hyperparameter for KNN method? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data should be approximately an increasing function of k, and will be 0 if k=1. So we need an independent data to test the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to enhance the linear regression or the simplest KNN? (very high-level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Kernel methods use weights that decrease smoothly to zero with distance from the target point, rather than the eﬀective 0/1 weights used by k-nearest neighbors.\n",
    "2. Local regression ﬁts linear models by locally weighted least squares, rather than ﬁtting constants locally.\n",
    "3. Linear models ﬁt to a basis expansion of the original inputs allow arbitrarily complex models.\n",
    "4. Projection pursuit and neural network models consist of sums of non-linearly transformed linear models.( more general case of 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between the predction of regression and knn? ( How to choose between two methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction for the regression is the conditional mean at any point X=x, when best is measured by average squared error.The prediction of knn is the average observation of k points whose features are closest to x. \n",
    "\n",
    "When $N, k \\to \\infty, s.t. \\frac{k}{N} \\to 0$, the prediction of KNN is approximately equal to the conditional expectation. However, when the data size is small and linear assumption is not violated, a linear regression can be more appropriate. \n",
    "\n",
    "Alternative explanation:\n",
    "1. OLS assumes $f(x)$ is approximated by a globally linear function\n",
    "2. knn assumes $f(x)$ is approximated by a locally constant function\n",
    "\n",
    "bias vs. variance\n",
    "* regression: stable but biased\n",
    "* knn: vice versa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we change *least squares* to *least abs value*? (L2 loss $\\to$ L1 Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we used median of k points in knn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we use L2 loss more than L1 Loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 criteria have discontinuities in their derivatives,while the squared error is anatytically convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of *curse of dimensionality* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P22 bottom. Need to be summarised. The second example is a very good brainteaser."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
